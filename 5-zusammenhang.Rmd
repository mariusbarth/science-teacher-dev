# Zusammenhänge zwischen zwei Variablen

Häufig möchte man nicht nur wissen, wie eine einzelne Variable verteilt ist, sondern vielmehr,
wie zwei oder mehr Variablen miteinander *zusammenhängen*.
<!-- Merkmale/Merkmalsträger wäre besser -->
Der Zusammenhang zweier Variablen meint deren gemeinsames Variieren (ihre *Kovariation*) --
also ein gemeinsames Auftreten von hohen oder niedrigen Werten:
Ein *gleichsinniger* oder *positiver* Zusammenhang bedeutet, dass hohe Werte in der einen Variable
mit hohen Werten in der anderen Variable einhergehen;
ein *gegenläufiger* oder *negativer* Zusammenhang liegt dann vor, wenn hohe Werte in der einen Variable
mit niedrigen Werte in der anderen Variable einhergen.

Um Zusammenhänge bildlich darzustellen, verwendet man häufig ein *Streudiagramm* wie in Abbildung \@ref(fig:first-scatterplot).

```{r first-scatterplot, fig.cap = "Der Zusammenhang zwischen der Geschwindigkeit eines Fahrzeugs und dem Bremsweg, um das Fahrzeug aus dieser Geschwindigkeit zum Stillstand zu bringen. Die Daten stammen aus den 1920er Jahren.", fig.width=5, fig.height = 4.4, fig.align='center'}
data <- cars
variable_label(data) <- c(speed = "Geschwindigkeit [mph]", dist = "Bremsweg [ft]")
out <- plot_bivariate(data$speed, data$dist, model = FALSE)
```

Neben der *Richtung* (positiv vs. negativ) eines Zusammenhangs ist auch die *Form* eines Zusammenhangs relevant.
Abbildung \@ref(fig:covariation-shape) zeigt einerseits, wie ein positiver im Vergleich zu einem negativen Zusammenhang aussieht (Tafel A vs. B),
aber auch wie ein *Nullzusammenhang* oder ein quadratischer Zusammenhang aussehen könnten. Darüber hinaus sind natürlich viele andere Formen, z.B. kubische oder umgekehrt-U-förmige Zusammenhänge möglich.

(ref:cap-shape) Vier unterschiedliche Formen von Zusammenhängen. Tafel A zeigt einen positiv-linearen, Tafel B einen negativ-linearen Zusammenhang.
Tafel C zeigt einen Nullzusammenhang -- es besteht kein Zusammenhang zwischen $X$ und $Y$. Tafel D zeigt einen quadratischen Zusammenhang.

```{r covariation-shape, fig.width = 8, fig.height = 8, fig.cap = "(ref:cap-shape)"}
set_shinydegs_theme()
par(mfrow = c(2, 2))
# ------------------------------------------------------------------------------
# positiv linear
x <- rnorm(1e4)
y <- scale(x * .5 + rnorm(1e4, mean = 0, sd = sqrt(.75)))
variable_label(x) <- ""
variable_label(y) <- ""
mod <- plot_bivariate(x = x, y = y, xlim = c(-4, 4), ylim = c(-4, 4), model = FALSE)
title(xlab = expression(paste("Variable"~italic(X))), ylab = expression(paste("Variable"~italic(Y))))
# lm(formula = y ~ x)
legend(x = 2, y = 6, legend = "A", bty = "n", cex = 2, xpd = NA)
# ------------------------------------------------------------------------------
# negativ linear
x <- rnorm(1e4)
y <- scale(-x * .5 + rnorm(1e4, mean = 0, sd = sqrt(.75)))
variable_label(x) <- ""
variable_label(y) <- ""
mod <- plot_bivariate(x = x, y = y, xlim = c(-4, 4), ylim = c(-4, 4), model = FALSE)
title(xlab = expression(paste("Variable"~italic(X))), ylab = expression(paste("Variable"~italic(Y))))
# lm(formula = y ~ x)
legend(x = 2, y = 6, legend = "B", bty = "n", cex = 2, xpd = NA)

# ------------------------------------------------------------------------------
# kein Zusammenhang
x <- rnorm(1e4)
y <- rnorm(1e4)

# lm(formula = y ~ x^2)
variable_label(x) <- ""
variable_label(y) <- ""
mod <- plot_bivariate(x = x, y = y, xlim = c(-4, 4), ylim = c(-4, 4), model = FALSE)
title(xlab = expression(paste("Variable"~italic(X))), ylab = expression(paste("Variable"~italic(Y))))
legend(x = 2, y = 6, legend = "C", bty = "n", cex = 2, xpd = NA)

# ------------------------------------------------------------------------------
# quadratisch
x <- rnorm(1e4)
y <- scale(x^2 * .5 + rnorm(1e4, mean = 0, sd = sqrt(.75)))

# lm(formula = y ~ x^2)
variable_label(x) <- ""
variable_label(y) <- ""
mod <- plot_bivariate(x = x, y = y, xlim = c(-4, 4), ylim = c(-6, 6), model = FALSE)
title(xlab = expression(paste("Variable"~italic(X))), ylab = expression(paste("Variable"~italic(Y))))
legend(x = 2, y = 6*1.5, legend = "D", bty = "n", cex = 2, xpd = NA)

# ------------------------------------------------------------------------------
# kubisch
# x <- rnorm(1e4)
# y <- scale(y <- x^3 * .5 + rnorm(1e4, mean = 0, sd = sqrt(.75)))
# 
# # lm(formula = y ~ x^3)
# variable_label(x) <- "Prädiktor"
# variable_label(y) <- "Kriterium"
# mod <- plot_bivariate(x = x, y = y, xlim = c(-4, 4), ylim = c(-12, 12), model = FALSE)
# title(xlab = expression(paste("Variable"~italic(X))), ylab = expression(paste("Variable"~italic(Y))))
# legend(x = 2, y = 12*1.5, legend = "D", bty = "n", cex = 2, xpd = NA)

```

Die dritte wichtige Eigenschaft eines Zusammenhangs ist dessen *Stärke*:
Sie drückt aus, wie *genau* man von den Werten der einen Variable auf die Werte der anderen Werte schließen kann.
<!-- Hiermit beschreibt man, wie eng das Verhältnis der beiden Variablen zueinander ist. -->
Die Stärke eines Zusammenhangs lässt sich mithilfe eines geeigneten statistischen Kennwerts quantifizieren.
Welcher statistische Kennwert in einer bestimmten Anwendung geeignet ist, hängt von der Form des Zusammenhangs,
dem Skalenniveau der beteiligten Variablen und weiteren Voraussetzungen einzelner statistischer Kennwerte bzw. Verfahren ab.
Die Frage, welcher Kennwert geeignet ist, stellt dabei eine der Fragen dar, mit denen sich ein großer Teil der statistischen Literatur beschäftigt,
den wir aber im Rahmen dieser Veranstaltung nicht vertiefen werden.

Ein besonders häufig verwendetes Maß für den Zusammenhang zweier Variablen ist der Korrelationskoeffizient $r$ nach Pearson
(er *eignet* sich zur Quantifizierung des linearen Zusammenhangs zweier intervallskalierter Variablen).
Er ist ein standardisiertes Maß für die *Richtung* und die *Stärke* eines linearen Zusammenhangs und
hat einen Wertebereich von -1 bis +1:
Negative Werte zeigen einen negativen, positive Werte zeigen einen positiven Zusammenhang an.
Ein Wert von 0 bedeutet, dass kein (linearer) Zusammenhang besteht.

Abbildung \@ref(fig:covariation-strength) zeigt unterschiedlich starke (linear-positive) Zusammenhänge.

(ref:cap-covariation-strength) Vier unterschiedlich *starke* Zusammenhänge (Die Gerade verdeutlicht die Stärke des linearen Zusammenhangs $r$).

```{r covariation-strength, fig.width = 8, fig.height = 8, fig.cap = "(ref:cap-covariation-strength)"}
set_shinydegs_theme()
par(mfrow = c(2, 2))

n_obs <- 2e3

r <- c(0.05, .35, .65, .95)

for (i in 1:4){
  # ------------------------------------------------------------------------------
  # positiv linear
  x <- rnorm(n_obs)
  y <- scale(x * r[i] + rnorm(n_obs, mean = 0, sd = sqrt(1-r[i]^2)))
  variable_label(x) <- ""
  variable_label(y) <- ""
  mod <- plot_bivariate(x = x, y = y, xlim = c(-3, 3), ylim = c(-3, 3), model = TRUE)
  title(xlab = expression(paste("Variable"~italic(X))), ylab = expression(paste("Variable"~italic(Y))))
  # lm(formula = y ~ x)
  legend(x = 2, y = 3*1.5, legend = substitute(italic(r)==a, list(a = papaja::printnum(cor(x, y), gt1=FALSE))), bty = "n", xpd = NA)
}

```

    
## Andere Formen von Zusammenhängen

Die obigen Beispiele zeigen ausschließlich Zusammenhänge zwischen zwei jeweils mindestens intervallskalierten (man spricht auch von *metrischen* oder *kardinalskalierten*) Variablen.
Zusammenhänge lassen sich aber auch sinnvoll zwischen Variablen unterschiedlicher Skalenniveaus bestimmen.
So zeigt zum Beispiel Abbildung \@ref(fig:logistic-regression) den Zusammenhang zwischen einer intervallskalierten Variable (hier der IQ-Wert)
und einer nominalskalierten Variable (hier die Zulassung zum Studium). Genauer ausgedrückt wird hier auf die $x$-Achse die intervallskalierte Variable gezeichnet, auf die $y$Achse zeichnen wir aber die *Wahrscheinlichkeit* dafür, dass eine bestimmte Ausprägung der nominalskalierten Variable vorliegt. In diesem (fiktiven) Beispiel ist es so, dass es einen Zusammenhang zwischen der Intelligenz und der Wahrscheinlichkeit zugelasse zu werden gibt.

(ref:cap-logistic-regression) Der Zusammenhang zwischen dem Abschneiden in einem allgemeinen Intelligenztest ($x$-Achse) und der Wahrscheinlichkeit, für
den Studiengang *Verteidigung gegen die dunklen Künste* ($y$-Achse) zugelassen zu werden.

(ref:cap-mean-differences) Der Zusammenhang zwischen der Zulassung zum Studiengang *Verteidigung gegen die dunklen Künste* ($x$-Achse) und dem Abschneiden in einem allgemeinen Intelligenztest ($y$-Achse). Für jeden

```{r logistic-regression, fig.cap = "(ref:cap-logistic-regression)"}
x <- round(rnorm(2e2, mean = 100, sd = 10))
y <- rbinom(n = 2e2, size = 1, prob = pnorm(x, mean = 105, sd = 10))
y <- factor(y, levels = 0:1, labels = c("keine Zulassung", "Zulassung"))
variable_label(x) <- "Intelligenz [IQ-Punkte]"
variable_label(y) <- "Entscheidung"

mod <- plot_bivariate(x = x, y = y, xlim = c(60, 140))
```

Abbildung \@ref(fig:mean-differences) zeigt die gleichen Daten bzw. auch den gleichen Zusammenhang:
Wie man sehen kann, kann man einen solchen Unterschied zwischen zwei Gruppen (Bewerber, die zugelassen wurden, sind auch im Mittel intelligenter) auch so verstehen, dass es einen *Zusammenhang zwischen Gruppenzugehörigkeit und Intelligenz gibt*.


```{r mean-differences, fig.cap = "(ref:cap-mean-differences)"}
apa_barplot(
  data = data.frame(x, y, id = 1:length(x))
  , id = "id"
  , dv = "x"
  , factors = "y"
  , args_error_bars = list(lwd = 0)
  , args_rect = getOption("shinydegs.theme")
  , las = 1
)
```


## Regression

Gibt es einen Zusammenhang zwischen zwei Variablen, kann man von den Werten der einen Variablen auf die Werte der anderen Variablen schließen.
Mit welcher *Genauigkeit* man dies kann, hängt von der *Stärke* des Zusammenhangs ab.

Wenn es einen Zusammenhang zwischen zwei Variablen gibt, kann man also aus der Kenntnis der Werte der einen
Variable auf die auf die Werte der anderen variable schließen oder die Werte der andereren Variable vorhersagen.
Wie genau man dies kann, hängt aber von der Stärke des Zusammenhangs ab.
Perfekte Korrelationen kommen aber in vielen Forschungsbereichen nicht vor, d.h.  meist kann man einen Wert der $Y$-Variable nicht perfekt
aus den Werten der $X$-Variable vorhersagen kann. Das bedeutet auch, dass man immer einen unterschiedlich großen *Vorhersagefehler* macht.

Die *Regressionsanalyse* ist eine Methode, diesen Vorhersagefehler zu minimieren.
Man spricht im Rahmen der Regressionsanalyse davon, eine *Kriteriumsvariable* (die $Y$-Variable)
durch eine oder mehrere *Prädiktorvariablen* (die $X$-Variable) vorherzusagen.
Die Regressionanalyse liefert eine sog. *Regressionsgleichung*, mit deren Hilfe man dann aus den Werten $x_i$ der Prädiktorvariable ($i$ ist der Laufindex, d.h. eine Zahl, doe angibt, um die wievielte Beobachtung es sich handelt)
vorhergesagte Werte $\hat{y}_i$ der Kriteriumsvariable berechnen kann (das kleine Dach über dem $y$ bedeutet, dass es sich um vorhergesagte und nicht um beobachtete Werte handelt).

Ihre allgemeine Form lautet:
$$\hat{y}_i = b_0 + b_1 * x_i$$

Die vorhergesagten Kriteriumswerte $\hat{y}_i$ sind also gleich der Summe aus einem $y$-Achsenabschnitt $b_0$ und dem Produkt eines Steigungskoeffizienten $b_1$ mit den Werten $x_i$ der Prädiktorvariable. Den $y$-Achsenabschnitt $b_0$ und den Steigungskoeffizienten $b_1$ nennt man auch Regressionskoeffizienten oder -gewichte. Diese Vorhersagegleichung lässt sich als Regressionsgerade in ein Streudigramm einzeichnen, wie es in Abbildung \@ref(fig:regression) darstellt ist.

```{r regression, fig.cap = "Der Zusammenhang zwischen der Geschwindigkeit eines Fahrzeugs und dem Bremsweg, um das Fahrzeug aus dieser Geschwindigkeit zum Stillstand zu bringen. Die Daten stammen aus den 1920er Jahren.", fig.width=5, fig.height = 4.4, fig.align='center'}
data <- cars
variable_label(data) <- c(speed = "Geschwindigkeit [mph]", dist = "Bremsweg [ft]")
out <- plot_bivariate(data$speed, data$dist, model = TRUE)
```



## Aufgaben {-}

1. Betrachtet die folgenden Zusammenhänge und vergleicht ihre Richtung, ihre Form, und ihre Stärke.

```{r covariation-shape-2, fig.width = 8, fig.height = 8}
set_shinydegs_theme()
par(mfrow = c(2, 2))
# ------------------------------------------------------------------------------
x <- rnorm(6e1)
y <- scale(x^2 * .1 + rnorm(6e1, mean = 0, sd = sqrt(1-.1^2)))

# lm(formula = y ~ x^2)
variable_label(x) <- ""
variable_label(y) <- ""
mod <- plot_bivariate(x = x, y = y, xlim = c(-4, 4), ylim = c(-6, 6), model = FALSE)
title(xlab = expression(paste("Variable"~italic(X))), ylab = expression(paste("Variable"~italic(Y))))
legend(x = 2, y = 6*1.5, legend = "A", bty = "n", cex = 2, xpd = NA)

# ------------------------------------------------------------------------------
x <- rnorm(2e2)
y <- scale(-x^2 * .6 + rnorm(2e2, mean = 0, sd = sqrt(1-.6^2)))
variable_label(x) <- ""
variable_label(y) <- ""
mod <- plot_bivariate(x = x, y = y, xlim = c(-4, 4), ylim = c(-4, 4), model = FALSE)
title(xlab = expression(paste("Variable"~italic(X))), ylab = expression(paste("Variable"~italic(Y))))
# lm(formula = y ~ x)
legend(x = 2, y = 6, legend = "B", bty = "n", cex = 2, xpd = NA)

# ------------------------------------------------------------------------------
# kubisch
x <- rnorm(1e4)
y <- scale(y <- x^3 * .5 + rnorm(1e4, mean = 0, sd = sqrt(.75)))

# lm(formula = y ~ x^3)
variable_label(x) <- ""
variable_label(y) <- ""
mod <- plot_bivariate(x = x, y = y, xlim = c(-4, 4), ylim = c(-12, 12), model = FALSE)
title(xlab = expression(paste("Variable"~italic(X))), ylab = expression(paste("Variable"~italic(Y))))
legend(x = 2, y = 12*1.5, legend = "C", bty = "n", cex = 2, xpd = NA)

# ------------------------------------------------------------------------------
x <- rnorm(6e1)
y <- scale(-x * .8 + rnorm(6e1, mean = 0, sd = sqrt(1-.8^2)))
variable_label(x) <- ""
variable_label(y) <- ""
mod <- plot_bivariate(x = x, y = y, xlim = c(-4, 4), ylim = c(-4, 4), model = FALSE)
title(xlab = expression(paste("Variable"~italic(X))), ylab = expression(paste("Variable"~italic(Y))))
# lm(formula = y ~ x)
legend(x = 2, y = 6, legend = "D", bty = "n", cex = 2, xpd = NA)
```

2. Betrachtet die folgende Darstellung einer Regression. Welchen $y$-Wert sagt die Regressionsgerade für eine Person der Größe 160 cm vorher?

```{r logistic-regression-2, fig.cap = "(ref:cap-logistic-regression)"}
par(mfrow = c(1, 1))
x <- rnorm(2e2, mean = 176, sd = 8)
y <- -40 + x * .3 + rnorm(2e2, mean = 0, sd = 3)
variable_label(x) <- "Körpergröße [cm]"
variable_label(y) <- "Armumfang [cm]"

mod <- plot_bivariate(x = x, y = y, xlim = c(150, 200))
```
