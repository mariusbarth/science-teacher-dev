# Zusammenhänge zwischen zwei Variablen

Häufig möchte man nicht nur wissen, wie eine einzelne Variable verteilt ist, sondern vielmehr,
wie zwei oder mehr Variablen miteinander *zusammenhängen*.
<!-- Merkmale/Merkmalsträger wäre besser -->
Der Zusammenhang zweier Variablen meint deren gemeinsames Variieren (ihre *Kovariation*) --
also ein gemeinsames Auftreten von hohen oder niedrigen Werten:
Ein *gleichsinniger* oder *positiver* Zusammenhang bedeutet, dass hohe Werte in der einen Variable
mit hohen Werten in der anderen Variable einhergehen;
ein *gegenläufiger* oder *negativer* Zusammenhang liegt dann vor, wenn hohe Werte in der einen Variable
mit niedrigen Werte in der anderen Variable einhergen.

Um Zusammenhänge bildlich darzustellen, verwendet man häufig ein *Streudiagramm* wie in Abbildung \@ref(fig:first-scatterplot).

```{r first-scatterplot, fig.cap = "Der Zusammenhang zwischen der Geschwindigkeit eines Fahrzeugs und dem Bremsweg, um das Fahrzeug aus dieser Geschwindigkeit zum Stillstand zu bringen. Die Daten stammen aus den 1920er Jahren.", fig.width=5, fig.height = 4.4, fig.align='center'}
data <- cars
variable_label(data) <- c(speed = "Geschwindigkeit [mph]", dist = "Bremsweg [ft]")
out <- plot_bivariate(data$speed, data$dist, model = FALSE)
```

Neben der *Richtung* (positiv--negativ) eines Zusammenhangs ist auch die *Form* eines Zusammenhangs relevant.
Abbildung \@ref(fig:covariation-shape) zeigt einerseits, wie ein positiver im Vergleich zu einem negativen Zusammenhang aussieht (Tafel A vs. B),
aber auch wie ein *Nullzusammenhang* oder ein quadratischer Zusammenhang aussehen könnten. Darüber hinaus sind natürlich viele andere Formen, z.B. kubische oder umgekehrt-U-förmige Zusammenhänge möglich.

(ref:cap-shape) Vier unterschiedliche Formen von Zusammenhängen. Tafel A zeigt einen positiv-linearen, Tafel B einen negativ-linearen Zusammenhang.
Tafel C zeigt einen Nullzusammenhang -- es besteht kein Zusammenhang zwischen $X$ und $Y$. Tafel D zeigt einen quadratischen Zusammenhang.

```{r covariation-shape, fig.width = 8, fig.height = 8, fig.cap = "(ref:cap-shape)"}
set_shinydegs_theme()
par(mfrow = c(2, 2))
# ------------------------------------------------------------------------------
# positiv linear
x <- rnorm(1e4)
y <- scale(x * .5 + rnorm(1e4, mean = 0, sd = sqrt(.75)))
variable_label(x) <- ""
variable_label(y) <- ""
mod <- plot_bivariate(x = x, y = y, xlim = c(-4, 4), ylim = c(-4, 4), model = FALSE)
title(xlab = expression(paste("Variable"~italic(X))), ylab = expression(paste("Variable"~italic(Y))))
# lm(formula = y ~ x)
legend(x = 2, y = 6, legend = "A", bty = "n", cex = 2, xpd = NA)
# ------------------------------------------------------------------------------
# negativ linear
x <- rnorm(1e4)
y <- scale(-x * .5 + rnorm(1e4, mean = 0, sd = sqrt(.75)))
variable_label(x) <- ""
variable_label(y) <- ""
mod <- plot_bivariate(x = x, y = y, xlim = c(-4, 4), ylim = c(-4, 4), model = FALSE)
title(xlab = expression(paste("Variable"~italic(X))), ylab = expression(paste("Variable"~italic(Y))))
# lm(formula = y ~ x)
legend(x = 2, y = 6, legend = "B", bty = "n", cex = 2, xpd = NA)

# ------------------------------------------------------------------------------
# kein Zusammenhang
x <- rnorm(1e4)
y <- rnorm(1e4)

# lm(formula = y ~ x^2)
variable_label(x) <- ""
variable_label(y) <- ""
mod <- plot_bivariate(x = x, y = y, xlim = c(-4, 4), ylim = c(-4, 4), model = FALSE)
title(xlab = expression(paste("Variable"~italic(X))), ylab = expression(paste("Variable"~italic(Y))))
legend(x = 2, y = 6, legend = "C", bty = "n", cex = 2, xpd = NA)

# ------------------------------------------------------------------------------
# quadratisch
x <- rnorm(1e4)
y <- scale(x^2 * .5 + rnorm(1e4, mean = 0, sd = sqrt(.75)))

# lm(formula = y ~ x^2)
variable_label(x) <- ""
variable_label(y) <- ""
mod <- plot_bivariate(x = x, y = y, xlim = c(-4, 4), ylim = c(-6, 6), model = FALSE)
title(xlab = expression(paste("Variable"~italic(X))), ylab = expression(paste("Variable"~italic(Y))))
legend(x = 2, y = 6*1.5, legend = "D", bty = "n", cex = 2, xpd = NA)

# ------------------------------------------------------------------------------
# kubisch
# x <- rnorm(1e4)
# y <- scale(y <- x^3 * .5 + rnorm(1e4, mean = 0, sd = sqrt(.75)))
# 
# # lm(formula = y ~ x^3)
# variable_label(x) <- "Prädiktor"
# variable_label(y) <- "Kriterium"
# mod <- plot_bivariate(x = x, y = y, xlim = c(-4, 4), ylim = c(-12, 12), model = FALSE)
# title(xlab = expression(paste("Variable"~italic(X))), ylab = expression(paste("Variable"~italic(Y))))
# legend(x = 2, y = 12*1.5, legend = "D", bty = "n", cex = 2, xpd = NA)

```

Die dritte wichtige Eigenschaft eines Zusammenhangs ist dessen *Stärke*:
<!-- Genauigkeit?? -->
Hiermit beschreibt man, wie eng das Verhältnis der beiden Variablen zueinander ist.
Die Stärke eines Zusammenhangs lässt sich mithilfe eines geeigneten statistischen Kennwerts quantifizieren.
Welcher statistische Kennwert in einer bestimmten Anwendung geeignet ist, hängt von der Form des Zusammenhangs,
dem Skalenniveau der beteiligten Variablen und weiteren Voraussetzungen einzelner statistischer Kennwerte bzw. Verfahren ab.
Die Frage, welcher Kennwert geeignet ist, stellt dabei eine der Fragen dar, mit denen sich ein großer Teil der statistischen Literatur beschäftigt,
den wir aber im Rahmen dieser Veranstaltung nicht vertiefen werden.

Ein besonders häufig verwendetes Maß für den Zusammenhang zweier Variablen ist der Korrelationskoeffizient $r$ nach Pearson
(er *eignet* sich zur Quantifizierung des linearen Zusammenhangs zweier intervallskalierter Variablen).
Er ist ein standardisiertes Maß für die *Richtung* und die *Stärke* eines linearen Zusammenhangs und
hat einen Wertebereich von -1 bis +1:
Negative Werte zeigen einen negativen, positive Werte zeigen einen positiven Zusammenhang an.
Ein Wert von 0 bedeutet, dass kein (linearer) Zusammenhang besteht.

Abbildung \@ref(fig:covariation-strength) zeigt unterschiedlich starke (linear-positive) Zusammenhänge.

(ref:cap-covariation-strength) Vier unterschiedlich *starke* Zusammenhänge.

```{r covariation-strength, fig.width = 8, fig.height = 8, fig.cap = "(ref:cap-covariation-strength)"}
set_shinydegs_theme()
par(mfrow = c(2, 2))

n_obs <- 2e3

r <- c(0.05, .35, .65, .95)

for (i in 1:4){
  # ------------------------------------------------------------------------------
  # positiv linear
  x <- rnorm(n_obs)
  y <- scale(x * r[i] + rnorm(n_obs, mean = 0, sd = sqrt(1-r[i]^2)))
  variable_label(x) <- ""
  variable_label(y) <- ""
  mod <- plot_bivariate(x = x, y = y, xlim = c(-3, 3), ylim = c(-3, 3), model = TRUE)
  title(xlab = expression(paste("Variable"~italic(X))), ylab = expression(paste("Variable"~italic(Y))))
  # lm(formula = y ~ x)
  legend(x = 2, y = 3*1.5, legend = substitute(italic(r)==a, list(a = papaja::printnum(cor(x, y), gt1=FALSE))), bty = "n", xpd = NA)
}

```

    
## Andere Formen von Zusammenhängen

Die obigen Beispiele zeigen ausschließlich Zusammenhänge zwischen zwei jeweils mindestens intervallskalierten (man spricht auch von *metrischen* oder *kardinalskalierten*) Variablen.
Zusammenhänge lassen sich aber auch sinnvoll zwischen Variablen unterschiedlicher Skalenniveaus bestimmen.
So zeigt zum Beispiel Abbildung \@ref(fig:logistic-regression) den Zusammenhang zwischen einer intervallskalierten Variable (hier der IQ-Wert)
und einer nominalskalierten Variable (hier die Zulassung zum Studium). Genauer ausgedrückt wird hier auf die $x$-Achse die intervallskalierte Variable gezeichnet, auf die $y$Achse zeichnen wir aber die *Wahrscheinlichkeit* dafür, dass eine bestimmte Ausprägung der nominalskalierten Variable vorliegt. In diesem (fiktiven) Beispiel ist es so, dass es einen Zusammenhang zwischen der Intelligenz und der Wahrscheinlichkeit zugelasse zu werden gibt.

(ref:cap-logistic-regression) Der Zusammenhang zwischen dem Abschneiden in einem allgemeinen Intelligenztest ($x$-Achse) und der Wahrscheinlichkeit, für
den Studiengang *Verteidigung gegen die dunklen Künste* ($y$-Achse) zugelassen zu werden.

(ref:cap-mean-differences) Der Zusammenhang zwischen der Zulassung zum Studiengang *Verteidigung gegen die dunklen Künste* ($x$-Achse) und dem Abschneiden in einem allgemeinen Intelligenztest ($y$-Achse). Für jeden

```{r logistic-regression, fig.cap = "(ref:cap-logistic-regression)"}
x <- round(rnorm(2e2, mean = 100, sd = 10))
y <- rbinom(n = 2e2, size = 1, prob = pnorm(x, mean = 105, sd = 10))
y <- factor(y, levels = 0:1, labels = c("keine Zulassung", "Zulassung"))
variable_label(x) <- "Intelligenz [IQ-Punkte]"
variable_label(y) <- "Entscheidung"

mod <- plot_bivariate(x = x, y = y, xlim = c(60, 140))
```

Abbildung \@ref(fig:mean-differences) zeigt die gleichen Daten bzw. auch den gleichen Zusammenhang:
Wie man sehen kann, kann man einen solchen Unterschied zwischen zwei Gruppen (Bewerber, die zugelassen wurden, sind auch im Mittel intelligenter) auch so verstehen, dass es einen *Zusammenhang zwischen Gruppenzugehörigkeit und Intelligenz gibt*.


```{r mean-differences, fig.cap = "(ref:cap-mean-differences)"}
apa_barplot(
  data = data.frame(x, y, id = 1:length(x))
  , id = "id"
  , dv = "x"
  , factors = "y"
  , args_error_bars = list(lwd = 0)
  , args_rect = getOption("shinydegs.theme")
  , las = 1
)
```


## Regression

Hierdurch ist es nun also auch möglich, von den Werten der einen Variablen auf die Werte der anderen Variablen schließen.
Mit welcher *Genauigkeit* man dies kann, hängt von der *Stärke* des Zusammenhangs ab.

Wenn es einen Zusammenhang zwischen zwei Variablen gibt, kann man also aus der Kenntnis der Werte der einen
Variable auf die auf die Werte der anderen variable schließen oder die Werte der andereren Variable vorhersagen.
Wie genau man dies kann, hängt aber von der Stärke des Zusammenhangs ab.
Perfekte Korrelationen kommen aber in vielen Forschungsbereichen nicht vor, d.h. immer dann, wenn man einen Wert der Y-Vari

Wollen wir die Werte

Kriterium
Prädiktor

